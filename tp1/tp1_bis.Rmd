---
title: 'TP 1: Regresión ordinal'
author: "Nicolás Celie, Martín Peralta, Nicolás Ian Rozenberg"
date: '`r Sys.Date()`'
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, warning=FALSE, message=FALSE, results='hide'}
library(tidyverse)
library(MASS)
library(broom) 
library(janitor)   
library(rsample)
library(ggplot2)
```

## Ejercicio 1

Dado que al hacer el primer análisis exploratorio nos dimos cuenta que había valores de edad completamente irrisorios, sacamos los outliers del dataset para tener datos representativos. Luego de ello, realizamos la separación en train-test dejando una proporción 80/20.

```{r, warning=FALSE, message=FALSE, results='hide'}
set.seed(123)

df <- read.csv2("data/data.csv", sep="\t") |> 
  as.data.frame()

# Filtramos outliers
Q1 <- quantile(df$age, 0.25, na.rm = TRUE)
Q3 <- quantile(df$age, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

limite_inferior <- Q1 - 1.5 * IQR
limite_superior <- Q3 + 1.5 * IQR

df <- df  |>  
  filter(age<limite_superior,
         age>limite_inferior)

# Split Train - Test
split <- initial_split(df, prop = 0.8)

df_train <- training(split)
df_test  <- testing(split)
```

## Ejercicio 2

Se eligió la pregunta Q30: "I think horoscopes are fun."

## Ejercicio 3

Una forma de plantear el problema es mediante una regresión lineal donde la variable dependiente sea la respuesta en la escala Likert, y las covariables la edad y variables indicadoras del género. Es decir,

$$
Likert_i = \beta_{\text{intercept}} + \beta_{\text{masc}} \text{masc}_i + \beta_{\text{fem}} \text{fem}_i + \beta_{\text{edad}} \text{edad}_i + \epsilon_i
$$


El problema de este enfoque es que $Likert_i$ toma $1,...,5$ como posibles valores, pero modelo lineal común asume normalidad en los errores, y por lo tanto en la variable dependiente. En este caso, es una variable discreta con un rango de sólo 5 valores. Por otra parte, se asume que las distancias entre las respuestas son iguales. Por ejemplo, podría no tener sentido considerar que la diferencia entre "Totalmente en desacuerdo" y "En desacuerdo" sea la misma que "En desacuerdo" y "Neutro".


Otra forma es modelarlo mediante una regresión multinomial. En esta, se modela a la probabilidad de que el individuo $i$ responda la opción $j$ como

$$
\mathbb{P}(Likert_{i, j} = 1) = \text{Softmax}(z_i)_j
$$

donde $z_i$ es un vector en $\mathbb{R}^5$ tal que, para $1 \leq j \leq 5$,

$$
z_{i, j} = \beta_{\text{intercept, j}} + \beta_{\text{masc}, j} \text{masc}_i + \beta_{\text{fem}, j} \text{fem}_i + \beta_{\text{edad}, j} \text{edad}_i
$$
y $\text{Softmax} : \mathbb{R}^p \to \mathbb{R}^p$ es la función

$$
\text{Softmax}(z)_j = \frac{e^{z_j}}{\sum_{j=1}^{p} e^{z_j}}
$$
Sin embargo, dicho enfoque no es el más apropiado tampoco. Esto se debe a que la variable dependiente no es una variable categórica nominal. Las posibles respuestas tienen un orden, que no se está modelando.


## Ejercicio 4

El modelo de regresión ordinal es un modelo de clasificación que a diferencia de la regresión multinomial, permite tener en cuenta un orden de las categorías que puede tomar la variable dependiente $Y$, como ocurre con las escalas Likert. A diferencia de la regresión multinomial donde se busca estimar la función de probabilidad puntual de la distribución de categorías dados los datos mediante un modelo lineal generalizado, ahora se busca estimar la función de distribución acumulada de las categorías, teniendo en cuenta su orden. Es decir, supongamos que tenemos $K$ categorías. Dada $1 \leq j \leq K$ una categoría, se busca estimar $P(Y \leq j)$. La forma en la que se realiza esto es la siguiente:

$$
\widehat{P(Y \leq j)} := g(\theta_j - \mathbf{x}^\top \boldsymbol{\beta})
$$
donde los $\theta_j$ son parámetros denominados *umbrales*, que están retringidos a $\theta_1 < \theta_2 < \cdots < \theta_{K-1}$, $x$ son las covariables, y $\beta$ son los predictores. Además, $g:\mathbb{R} \to [0, 1]$ es una función de link. Si la función de link es la función logística, al modelo se lo conoce como modelo *logit*. Si la función de link es la función de distribución acumulada de una normal estándar, se lo conoce como *probit*.

## Ejercicio 5

Observamos que había valores que no estaban dentro de la escala (particularmente el 0), por lo tanto decidimos filtrarlos ya que no los consideramos correctos para Likert. Luego, ajustamos el modelo de regresión ordinal *logit*. No sin antes acondicionar la variable Q30, convirtiéndola en factor ordenado, para usarla en la función polr.

```{r, warning=FALSE}
df_train_30 <- df_train |> 
  filter(Q30 != 0)

df_test_30 <- df_test |> 
  filter(Q30 != 0)

#Ordenamos en escala
df_train_30$Q30_factor <- ordered(df_train_30$Q30,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

df_test_30$Q30_factor <- ordered(df_test_30$Q30,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_ordinal <- polr(Q30_factor ~ age, data = df_train_30, Hess = TRUE, method = "logistic")
summary(modelo_ordinal)
```
Obtenemos el ajuste del modelo, que nos devuelve los umbrales y el valor de $\beta$ para la variable age.
Vemos que:
$\beta = -0.024$ lo que sugiere que al aumentar la edad, la probabilidad de estar en niveles más altos en la escala Likert va disminuyendo. Es decir, es más probable que una persona más joven esté al menos "De Acuerdo" que una persona más vieja.
$t-value = -31.5$ lo que sugiere que la variable age tiene significancia considerable en la variable respuesta ya que posee un valor absoluto relativamente alto.

## Ejercicio 6

Ahora queremos estimar la probabilidad de que una persona de 25 años esté al menos de acuerdo con la frase "me gustan las armas" (pregunta 9). Por lo tanto, como es una pregunta diferente a la que nosotros elegimos, vamos a filtrar, desde el dataframe completo, los 0s para esta pregunta (ya que el anterior lo habíamos filtrado por los 0s para la pregunta Q30).

```{r, warning=FALSE}

# Entrenamos con todos los datos en este caso
df_9 <- df |> 
  filter(Q9 != 0)

df_9$Q9 <- ordered(df_9$Q9,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_9 <- polr(Q9 ~ age, data = df_9, Hess = TRUE, method = "logistic")
summary(modelo_9)
```
En este caso obtenemos los siguientes parámetros al ajustar:
$\beta = -0.003$ al igual que en el caso de la pregunta Q30, al aumentar la edad la probabilidad de estar en niveles más altos en la escala Likert va disminuyendo. Sin embargo, en este caso el valor de $\beta$ es más chico, lo que implica que la edad es menos significativa que en el caso anterior.

A continuación, usamos la función "predict", la cual estima la probabilidad puntual de cada opción y luego sumamos las instancias "De acuerdo" y "Muy de acuerdo" para responder a la pregunta enunciada.

```{r}
probabilidades <- predict(modelo_9, newdata = data.frame(age = 25), type = "probs")
print(probabilidades)
DeAcuerdo <- probabilidades["De acuerdo"]
MuyDeAcuerdo <- probabilidades["Muy de acuerdo"]
print(paste0("La probabilidad de estar al menos de acuerdo para una nueva persona de 25 años: ", as.numeric(DeAcuerdo + MuyDeAcuerdo)))
```
## Ejercicio 7

Implementamos la función de pérdida:

$$
L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|
$$

La misma corresponde a la función MAE (Mean Absolute Error).

```{r}
mae <- function(y_true, y_pred) {
  return(mean(abs(y_true - y_pred)))
}
```

## Ejercicios 8 y 9

Entrenamos y predecimos con el modelo lineal truncado (ejercicio 8)

```{r, warning=FALSE, message=FALSE, results='hide'}
modelo_lineal <- lm(Q30 ~ age, data = df_train_30)
y_pred_train <- predict(modelo_lineal)
y_pred_final_train <- pmin(pmax(round(y_pred_train), 1), 5)
mae_train <- mean(abs(df_train_30$Q30 - y_pred_final_train))
aciertos_train <- mean(df_train_30$Q30 == y_pred_final_train)

y_pred_test <- predict(modelo_lineal, newdata = df_test_30)
y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)
mae_test <- mean(abs(df_test_30$Q30 - y_pred_final_test))
aciertos_test <- mean(df_test_30$Q30 == y_pred_final_test)
print("Resultados para modelo lineal truncado:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```
Obtenemos así, los siguientes resultados:


| **Métrica**  | **Train** | **Test** |
|--------------|-----------|----------|
| MAE          | 1.3       | 1.29     |
| Exactitud    | 0.18      | 0.19     |

**Tabla:** Regresión Lineal Truncada: Resultados de MAE y Exactitud en los conjuntos de entrenamiento y prueba.


Entrenamos y predecimos con el modelo de regresión ordinal (ejercicio 9)

```{r, warning=FALSE, message=FALSE, results='hide'}
y_pred_train <- as.numeric(predict(modelo_ordinal))
mae_train <- mean(abs(df_train_30$Q30 - y_pred_train))
aciertos_train <- mean(df_train_30$Q30 == y_pred_train)

y_pred_test <- as.numeric(predict(modelo_ordinal, newdata = df_test_30))
# y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)   innecesario, y_pred_test ya es un 'factor', no es como el modelo lineal
mae_test <- mean(abs(df_test_30$Q30 - y_pred_test))
aciertos_test <- mean(df_test_30$Q30 == y_pred_test)
print("Resultados para modelo ordinal logístico:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```
| **Métrica**  | **Train** | **Test** |
|--------------|-----------|----------|
| MAE          | 1.95      | 1.96     |
| Exactitud    | 0.27      | 0.27     |

**Tabla:** Regresión Ordinal: Resultados de MAE y Exactitud en los conjuntos de entrenamiento y prueba.

Los modelos tuvieron un mal desempeño, dado que tener una distancia mayor a 1 en la escala Likert puede ser un cambio de respuesta de una categoría a otra y eso puede significar pasar, por ejemplo, de un "Neutro" a un "De acuerdo" (un abismo de diferencia). 
Vemos también que el modelo lineal truncado resultó tener un mejor MAE que el modelo ordinal, lo que es a priori extraño dado que el modelo lineal no tiene una estructura compatible con la escala Likert como sí lo tiene el modelo ordinal. 

Habiendo visto estos resultados, es posible que esté ocurriendo alguna de estas dos: o bien, hay un desbalance entre clases o la edad no es un buen predictor de 'Q30'.

Con esto en mente, en primer lugar decidimos ver la distribución de edades por respuesta.


```{r}

# Boxplot
boxplot(age ~ Q30, data = (df |> filter(Q30 != 0)),
        #col = c("#FFA07A", "#90EE90", ),
        main = "Distribución de edad por respuesta a Q30",
        ylab = "Edad",
        xlab = "Respuesta")
```

Acá se ve que, a pesar de que las medianas de las distribuciones decrecen levemente, no hay una diferencia clara por respuesta. Es decir, la edad explica poco de la varianza de la variable respuesta. Por lo tanto, es razonable que ninguno de los dos modelos (el lineal truncado y el ordinal) dé buenos resultados. Por otra parte, veamos la distribución de las clases en el conjunto de test, para ver si hay algún desbalance importante.

```{r}
hist(df_test_30$Q30, main="Distribución de clases de Q30", xlab = "Respuesta")
```

A pesar de que hay un desbalance de clases especialmente entre las respuestas **1** y **2**, no parecería que el problema viene por este lado. Por lo tanto, para verificar que el problema está en la covariable, decidimos crear datos sintéticos donde se observen diferencias en las distribuciones de edades, y ver si el modelo ajusta de manera razonable a los datos. Para descartar que sea un problema  del desbalance de clases, se decidió crear la misma distribución de clases que el conjunto de test.

```{r}
set.seed(123)

n_total <- 1000

#n_per_group <- n_total / 5
n_per_group <- n_total*table(df_test_30$Q30)/nrow(df_test_30)

likert_levels <- 1:5
age_means <- c(95, 70, 50, 30, 15)
age_sds   <- c(5, 10, 8, 6, 3)

data_list <- lapply(1:5, function(i) {
  ages <- rnorm(n_per_group[i], mean = age_means[i], sd = age_sds[i])
  ages <- pmin(pmax(ages, 10), 110)
  data.frame(
    Q_synthetic = rep(likert_levels[i], n_per_group[i]),
    age = ages
  )
})

df_synthetic <- do.call(rbind, data_list)

boxplot(age ~ Q_synthetic, data = df_synthetic, main = "Edad por respuesta Likert", xlab = "Q_synthetic", ylab = "Edad")

split <- initial_split(df_synthetic, prop = 0.8, strata = Q_synthetic)
train_data_synth <- training(split)
test_data_synth  <- testing(split)

train_data_synth$Q_synthetic <- factor(train_data_synth$Q_synthetic, ordered = TRUE)
test_data_synth$Q_synthetic  <- factor(test_data_synth$Q_synthetic, ordered = TRUE)

modelo_ordinal_synth <- polr(Q_synthetic ~ age, data = train_data_synth, Hess = TRUE)

preds_synth <- predict(modelo_ordinal_synth, newdata = test_data_synth)
mae_synth <- mae(as.numeric(preds_synth), as.numeric(test_data_synth$Q_synthetic))

```
El MAE obtenido en el conjunto de test es de
```{r}
cat("MAE:", mae_synth, "\n")
```

Esto implica un buen ajuste para los datos sintéticos, lo que supone que el principal problema viene por la capacidad predictiva de $\beta$.
En el anexo 1, exploramos las demás preguntas para ver si había alguna donde la edad explicara un poco mejor la varianza de las respuestas, con el objetivo de ver si en algún caso, las métricas del modelo ordinal dieran mejor que en el modelo lineal truncado.

### Ejercicio 10

### Ejercicio 11

Para este ejercicio, en vez de la biblioteca `rstan` vamos a utilizar `cmdstanr`, que es una interfaz de Stan para R equivalente. Ahora, tenemos la libertad de elegir la distribución que queramos directamente para los parámetros. En este caso, eligiremos en primer lugar priors que no funcionarían para el caso de la pregunta Q30, para el coeficiente $\beta$, donde se vio una relación débilmente decreciente entre la edad y la respuesta. Estas son una normal de media $15$ y varianza $1$, y una exponencial con media $10$, que tiene el problema en este caso de que su soporte son los valores positivos. Luego, se ajustará con tres priors normales centrados en $0$, pero con varianzas distintas. El prior se los puntos de corte $\theta_i$ se fijará en una normal estándar.
Decidimos hacer un subsample a 5000 observaciones, para que la corrida de MCMC.

```{r}
create_ordinal_regression_stan <- function(prior_beta, prior_theta){
  ordinal_regression_stan <- glue("
  data {{
    int<lower=0> N; // Cantidad de observaciones
    array[N] int<lower=1, upper=5> y;
    array[N] real x;
  }}
  parameters {{
    vector[1] beta;
    ordered[4] theta; // 4 puntos de corte
  }}
  model {{
    beta ~ {prior_beta};
    theta ~ {prior_theta}; // Todos los thetas tienen el mismo prior
    for (n in 1:N)
      y[n] ~ ordered_logistic(x[n] * beta[1], theta);
  }}
  ")
  ordinal_regression_stan
}

prior_beta_alejado <- "normal(15, 1)"
prior_beta_positivo <- "exponential(1/10)"
prior_beta_centrado_fuerte <- "normal(0, 1)"
prior_beta_centrado_moderado<- "normal(0,5)"
prior_beta_centrado_debil <- "normal(0, 20)"
prior_theta <- "normal(0,1)"


# Lista de priors
priors_beta <- list(
  "Normal(15,1)" = prior_beta_alejado,
  "Exponencial media 10" = prior_beta_positivo,
  "Normal(0,1)" = prior_beta_centrado_fuerte,
  "Normal(0,5)" = prior_beta_centrado_moderado,
  "Normal(0,20)" = prior_beta_centrado_debil
)

# Filtramos respuestas válidas
df_filtered <- df %>%
  filter(Q30 != 0)

# Subsample estratificado
set.seed(123)
df_strat <- df_filtered %>%
  group_by(Q30) %>%
  sample_frac(size = 5000 / nrow(df_filtered)) %>%
  ungroup()

# Definimos la lista de datos para Stan
data_list <- list(
  N = nrow(df_strat),
  y = df_strat$Q30,
  x = as.numeric(df_filtered[["age"]])
)

mod <- stan_model(model_code = create_ordinal_regression_stan("normal(0,1)", "normal(0,1)"))
cat(create_ordinal_regression_stan("normal(0,1)", "normal(0,1)"))

posterior_betas <- data.frame()

for (i in seq_along(priors_beta)) {
  prior_name <- names(priors_beta)[i]
  prior_code <- priors_beta[[i]]

  print(paste("Ajustando modelo con prior:", prior_name))

  code <- create_ordinal_regression_stan(prior_code, prior_theta)
  mod <- stan_model(model_code = code)
  fit <- stan(mod, data = data_list, seed = 123, iter=2000, chains=4, output_samples = 1000, refresh = 10)

  beta_df <- data.frame(beta = extract(fit)$beta, prior = prior_name)
  posterior_betas <- rbind(posterior_betas, beta_df)
}

```


### Anexo 1: búsqueda de otra pregunta
Ahora, podemos buscar alguna pregunta que sí muestre una diferencia más grande en las distribuciones de las edades, por respuesta. Para eso, podemos correr este programa (en vez de mirar un boxplots por cada pregunta) que nos ordene a las preguntas en base a la diferencia de edad que tiene la población de acuerdo y la que no. Para comparar las edades de las poblaciones, vamos a intentar únicamente comparando medias (puede que no haga falta más que esto).

```{r}
max_dif <- 0
max_dif_col <- ""

for (col in colnames(df)[1:44]) {
  df$grupos <- ifelse(df[[col]] %in% 4:5, "De acuerdo", "No de acuerdo")
  df_box <- df[!is.na(df$age) & !is.na(df[[col]]), ]
  media_dif <- abs(mean(df_box$age[df_box$grupos == "De acuerdo"]) - mean(df_box$age[df_box$grupos == "No de acuerdo"]))
  if (media_dif > max_dif) {
    max_dif_col <- col
    max_dif <- media_dif
  }
}

print(max_dif_col)
print(max_dif)

```

El programa indica que la pregunta Q23 ("I playfully insult my friends") maximiza la diferencia de medias de las poblaciones a favor y en contra. Veámoslo en el mismo gráfico que hicimos para Q30.

```{r}
df$Q23_grupo <- ifelse(df$Q23 %in% 4:5, "De acuerdo (4 o 5)", "No de acuerdo (1 a 3)")
df_box <- df[!is.na(df$age) & !is.na(df$Q23_grupo), ]

boxplot(age ~ Q23_grupo, data = df_box,
        col = c("#FFA07A", "#90EE90"),
        main = "Distribución de edad por respuesta a Q23",
        ylab = "Edad",
        xlab = "Grupo de respuesta")

table(df$Q23_grupo)
```
Podemos observar que ahora sí hay una diferencia entre las edades de las poblaciones de acuerdo y en desacuerdo. Veamos si los modelos predicen mejor la adhesión o no a esta pregunta 'Q23'.
```{r}
df_train_23 <- df_train |> 
  filter(Q23 != 0)

df_test_23 <- df_test |> 
  filter(Q23 != 0)

# Ordenamos en escala
df_train_23$Q23_factor <- ordered(df_train_23$Q23,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

df_test_23$Q23_factor <- ordered(df_test_23$Q23,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_ordinal_23 <- polr(Q23_factor ~ age, data = df_train_23, Hess = TRUE, method = "logistic")
summary(modelo_ordinal_23)
```


```{r}
modelo_lineal <- lm(Q23 ~ age, data = df_train_23)
y_pred_train <- predict(modelo_lineal)
y_pred_final_train <- pmin(pmax(round(y_pred_train), 1), 5)
mae_train <- mean(abs(df_train_23$Q23 - y_pred_final_train))
aciertos_train <- mean(df_train_23$Q23 == y_pred_final_train)

y_pred_test <- predict(modelo_lineal, newdata = df_test_23)
y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)
mae_test <- mean(abs(df_test_23$Q23 - y_pred_final_test))
aciertos_test <- mean(df_test_23$Q23 == y_pred_final_test)
print("Resultados para modelo lineal truncado:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```


```{r}
y_pred_train <- as.numeric(predict(modelo_ordinal_23))
mae_train <- mean(abs(df_train_23$Q23 - y_pred_train))
aciertos_train <- mean(df_train_23$Q23 == y_pred_train)

y_pred_test <- as.numeric(predict(modelo_ordinal_23, newdata = df_test_23))
# y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)   innecesario, y_pred_test ya es un 'factor', no es como el modelo lineal
mae_test <- mean(abs(df_test_23$Q23 - y_pred_test))
aciertos_test <- mean(df_test_23$Q23 == y_pred_test)
print("Resultados para modelo ordinal logístico:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```

¿Y si en vez de medir la exactitud en base a qué porcentaje de la población el modelo predijo exactamente su respuesta (1 a 5), intentaramos medir a cuántos le acierta sobre si están al menos a favor o no? 

```{r}

# Verdadero sentimiento
grupo_real_train <- ifelse(df_train_23$Q23 %in% 4:5, "a_favor", "no_a_favor")
grupo_real_test  <- ifelse(df_test_23$Q23  %in% 4:5, "a_favor", "no_a_favor")

# Predicción de sentimiento
grupo_pred_train <- ifelse(y_pred_train %in% 4:5, "a_favor", "no_a_favor")
grupo_pred_test  <- ifelse(y_pred_test  %in% 4:5, "a_favor", "no_a_favor")

# 3. Exactitud en clasificar a favor vs no a favor
acierto_a_favor_train <- mean(grupo_real_train == grupo_pred_train)
acierto_a_favor_test  <- mean(grupo_real_test  == grupo_pred_test)

# 4. Mostrar todo
print("Resultados para modelo ordinal logístico:")
list(
  MAE_train = mae_train,
  MAE_test = mae_test,
  exactitud_train = aciertos_train,
  exactitud_test = aciertos_test,
  exactitud_grupo_train = acierto_a_favor_train,
  exactitud_grupo_test = acierto_a_favor_test
)
```



