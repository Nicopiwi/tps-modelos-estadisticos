---
title: 'TP 1: Regresión ordinal'
author: "Nicolás Celie, Martín Peralta, Nicolás Ian Rozenberg"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(MASS)
library(broom) 
library(janitor)   
library(rsample)

```

## Ejercicio 1

```{r libraries}
set.seed(123)

df <- read.csv2("data/data.csv", sep="\t") %>%
  as.data.frame()

# Filtramos outliers
Q1 <- quantile(df$age, 0.25, na.rm = TRUE)
Q3 <- quantile(df$age, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

limite_inferior <- Q1 - 1.5 * IQR
limite_superior <- Q3 + 1.5 * IQR

nrow(df %>% 
  filter(age>limite_superior | age<limite_inferior)
  )

df <- df %>% 
  filter(age<limite_superior & age>limite_inferior)

# Split stratificado
df$strata <- interaction(df$religion, df$gender, df$education, drop = TRUE)

split <- initial_split(df, prop = 0.8, strata = strata)

df_train <- training(split)
df_test  <- testing(split)
```

## Ejercicio 2

Se eligió la pregunta Q30: 

## Ejercicio 3

Una forma de plantear el problema es mediante una regresión lineal donde la variable dependiente sea la respuesta en la escala Likert, y las covariables la edad y variables indicadoras del género. Es decir,

$$
Likert_i = \beta_{\text{intercept}} + \beta_{\text{masc}} \text{masc}_i + \beta_{\text{fem}} \text{fem}_i + \beta_{\text{edad}} \text{edad}_i + \epsilon_i
$$


El problema de este enfoque es que $Likert_i$ toma $1,...,5$ como posibles valores, pero modelo lineal común asume normalidad en los errores, y por lo tanto en la variable dependiente. En este caso, es una variable discreta con un rango de sólo 5 valores. Por otra parte, se asume que las distancias entre las respuestas son iguales. Por ejemplo, podría no tener sentido considerar que la diferencia entre "Totalmente en desacuerdo" y "En desacuerdo" sea la misma que "En desacuerdo" y "Neutro".


Otra forma es modelarlo mediante una regresión multinomial. En esta, se modela a la probabilidad de que el individuo $i$ responda la opción $j$ como

$$
\mathbb{P}(Likert_{i, j} = 1) = \text{Softmax}(z_i)_j
$$

donde $z_i$ es un vector en $\mathbb{R}^5$ tal que, para $1 \leq j \leq 5$,

$$
z_{i, j} = \beta_{\text{intercept, j}} + \beta_{\text{masc}, j} \text{masc}_i + \beta_{\text{fem}, j} \text{fem}_i + \beta_{\text{edad}, j} \text{edad}_i
$$
y $\text{Softmax} : \mathbb{R}^p \to \mathbb{R}^p$ es la función

$$
\text{Softmax}(z)_j = \frac{e^{z_j}}{\sum_{j=1}^{p} e^{z_j}}
$$
Sin embargo, dicho enfoque no es el más apropiado tampoco. Esto se debe a que la variable dependiente no es una variable categórica nominal. Las posibles respuestas tienen un orden, que no se está modelando.


## Ejercicio 4

El modelo de regresión ordinal es un modelo de clasificación que a diferencia de la regresión multinomial, permite tener en cuenta un orden de las categorías que puede tomar la variable dependiente $Y$, como ocurre con las escalas Likert. A diferencia de la regresión multinomial donde se busca estimar la función de probabilidad puntual de la distribución de categorías dados los datos mediante un modelo lineal generalizado, ahora se busca estimar la función de distribución acumulada de las categorías, teniendo en cuenta su orden. Es decir, supongamos que tenemos $K$ categorías. Dada $1 \leq j \leq K$ una categoría, se busca estimar $P(Y \leq j)$. La forma en la que se realiza esto es la siguiente:

$$
\widehat{P(Y \leq j)} := g(\theta_j - \mathbf{x}^\top \boldsymbol{\beta})
$$
donde los $\theta_j$ son parámetros denominados *umbrales*, que están retringidos a $\theta_1 < \theta_2 < \cdots < \theta_{K-1}$, $x$ son las covariables, y $\beta$ son los predictores. Además, $g:\mathbb{R} \to [0, 1]$ es una función de link. Si la función de link es la función logística, al modelo se lo conoce como modelo *logit*. Si la función de link es la función de distribución acumulada de una normal estándar, se lo conoce como *probit*.

## Ejercicio 5


```{r}


df_train_30 <- df_train %>% 
  filter(Q30 != 0)

df_test_30 <- df_test %>% 
  filter(Q30 != 0)

#Ordenamos en escala
df_train_30$Q30_factor <- ordered(df_train_30$Q30,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

df_test_30$Q30_factor <- ordered(df_test_30$Q30,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_ordinal <- polr(Q30_factor ~ age, data = df_train_30, Hess = TRUE, method = "logistic")
summary(modelo_ordinal)
```
## Ejercicio 6

```{r}

# Entrenamos con todos los datos en este caso
df_9 <- df %>% 
  filter(Q9 != 0)

df_9$Q9 <- ordered(df_9$Q9,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_9 <- polr(Q9 ~ age, data = df_9, Hess = TRUE, method = "logistic")
summary(modelo_9)
```
La función `predict` estima la probabilidad puntual de cada opción.

```{r}
probabilidades <- predict(modelo_9, newdata = data.frame(age = 25), type = "probs")
print(probabilidades)
```
TODO: Sumar 'De acuerdo' con 'Muy de acuerdo'

## Ejercicio 7

```{r}
mae <- function(y_true, y_pred) {
  return(mean(abs(y_true - y_pred)))
}
```

## Ejercicios 8 y 9

Entrenamos y predecimos con el modelo lineal truncado (ejercicio 8)

```{r}
modelo_lineal <- lm(Q30 ~ age, data = df_train_30)
y_pred_train <- predict(modelo_lineal)
y_pred_final_train <- pmin(pmax(round(y_pred_train), 1), 5)
mae_train <- mean(abs(df_train_30$Q30 - y_pred_final_train))
aciertos_train <- mean(df_train_30$Q30 == y_pred_final_train)

y_pred_test <- predict(modelo_lineal, newdata = df_test_30)
y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)
mae_test <- mean(abs(df_test_30$Q30 - y_pred_final_test))
aciertos_test <- mean(df_test_30$Q30 == y_pred_final_test)
print("Resultados para modelo lineal truncado:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```

Entrenamos y predecimos con el modelo de regresión ordinal (ejercicio 9)


```{r}
y_pred_train <- as.numeric(predict(modelo_ordinal))
mae_train <- mean(abs(df_train_30$Q30 - y_pred_train))
aciertos_train <- mean(df_train_30$Q30 == y_pred_train)

y_pred_test <- as.numeric(predict(modelo_ordinal, newdata = df_test_30))
# y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)   innecesario, y_pred_test ya es un 'factor', no es como el modelo lineal
mae_test <- mean(abs(df_test_30$Q30 - y_pred_test))
aciertos_test <- mean(df_test_30$Q30 == y_pred_test)
print("Resultados para modelo ordinal logístico:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```


Los modelos tuvieron un mal desempeño. Además de que solo tienen una variable para predecir, lo más probable es que esté pasando una de estas dos cosas (o ambas): o hay poca varianza en las respuestas a 'Q30' o la edad no es un buen predictor de 'Q30'.

Con esto en mente, decidimos ver qué porcentaje de personas respondió cada valor de la escala para 'Q30' y también visualizar la distribución de las edades de las personas que al menos estaban de acuerdo vs las que no.

```{r}
# Tabulación y porcentaje
tabla <- table(df$Q30)
porcentaje <- prop.table(tabla) * 100
porcentaje_formateado <- round(porcentaje, 2)
data.frame(Respuesta = names(tabla), Porcentaje = porcentaje_formateado)
```


```{r}
df$Q30_grupo <- ifelse(df$Q30 %in% 4:5, "De acuerdo (4 o 5)", "No de acuerdo (1 a 3)")
df_box <- df[!is.na(df$age) & !is.na(df$Q30_grupo), ]

# Boxplot
boxplot(age ~ Q30_grupo, data = df_box,
        col = c("#FFA07A", "#90EE90"),
        main = "Distribución de edad por respuesta a Q30",
        ylab = "Edad",
        xlab = "Grupo de respuesta")
```

Acá se ve claramente cómo casi no hay una diferencia clara por edad. Por lo tanto, es razonable que ninguno de los dos modelos (el lineal truncado y el ordinal) de buenos resultados.

TODO: Cambiar esto para que en vez de que la distribución de clases sea uniforme, tenga la misma distribución de clases que los datos reales de Q30. Para ver que no tiene que ver con eso tampoco.

Por lo tanto, para verificar que el problema está aquí, decidimos crear datos sintéticos donde se observen diferencias en las distribuciones de edades, y ver si el modelo ajusta de manera razonable a los datos.

```{r}
set.seed(123)  # Para reproducibilidad

n_total <- 1000

n_per_group <- n_total / 5

likert_levels <- 1:5
age_means <- c(95, 70, 50, 30, 15)
age_sds   <- c(5, 10, 8, 6, 3)

data_list <- lapply(1:5, function(i) {
  ages <- rnorm(n_per_group, mean = age_means[i], sd = age_sds[i])
  ages <- pmin(pmax(ages, 10), 110)
  data.frame(
    Q_synthetic = rep(likert_levels[i], n_per_group),
    age = ages
  )
})

df_synthetic <- do.call(rbind, data_list)

boxplot(age ~ Q_synthetic, data = df_synthetic, main = "Edad por respuesta Likert", xlab = "Q_synthetic", ylab = "Edad")

split <- initial_split(df_synthetic, prop = 0.8, strata = Q_synthetic)
train_data_synth <- training(split)
test_data_synth  <- testing(split)

train_data_synth$Q_synthetic <- factor(train_data_synth$Q_synthetic, ordered = TRUE)
test_data_synth$Q_synthetic  <- factor(test_data_synth$Q_synthetic, ordered = TRUE)

modelo_ordinal_synth <- polr(Q_synthetic ~ age, data = train_data_synth, Hess = TRUE)

preds_synth <- predict(modelo_ordinal_synth, newdata = test_data_synth)
mae_synth <- mae(as.numeric(preds_synth), as.numeric(test_data_synth$Q_synthetic))
cat("MAE:", mae_synth, "\n")
```
El MAE obtenido en el conjunto de test es de $0.14$. Esto implica un buen ajuste para los datos sintéticos, lo que supone que el principal problema viene por la capacidad predictiva de $\beta$.

Ahora, podemos buscar alguna pregunta que sí muestre una diferencia más contundente por edad. Para eso, podemos correr este programa (en vez de mirar un boxplots por cada pregunta) que nos ordene a las preguntas en base a la diferencia de edad que tiene la población de acuerdo y la que no. Para comparar las edades de las poblaciones, vamos a intentar únicamente comparando medias (puede que no haga falta más que esto).

```{r}
max_dif <- 0
max_dif_col <- ""

for (col in colnames(df)[1:44]) {
  df$grupos <- ifelse(df[[col]] %in% 4:5, "De acuerdo", "No de acuerdo")
  df_box <- df[!is.na(df$age) & !is.na(df[[col]]), ]
  media_dif <- abs(mean(df_box$age[df_box$grupos == "De acuerdo"]) - mean(df_box$age[df_box$grupos == "No de acuerdo"]))
  if (media_dif > max_dif) {
    max_dif_col <- col
    max_dif <- media_dif
  }
}

print(max_dif_col)
print(max_dif)

```

El programa indica que la pregunta Q23 ("I playfully insult my friends") maximiza la diferencia de medias de las poblaciones a favor y en contra. Veámoslo en el mismo gráfico que hicimos para Q30.

```{r}
df$Q23_grupo <- ifelse(df$Q23 %in% 4:5, "De acuerdo (4 o 5)", "No de acuerdo (1 a 3)")
df_box <- df[!is.na(df$age) & !is.na(df$Q23_grupo), ]

boxplot(age ~ Q23_grupo, data = df_box,
        col = c("#FFA07A", "#90EE90"),
        main = "Distribución de edad por respuesta a Q23",
        ylab = "Edad",
        xlab = "Grupo de respuesta")

table(df$Q23_grupo)
```
Podemos observar que ahora sí hay una diferencia entre las edades de las poblaciones de acuerdo y en desacuerdo. Veamos si los modelos predicen mejor la adhesión o no a esta pregunta 'Q23'.
```{r}
df_train_23 <- df_train %>% 
  filter(Q23 != 0)

df_test_23 <- df_test %>% 
  filter(Q23 != 0)

# Ordenamos en escala
df_train_23$Q23_factor <- ordered(df_train_23$Q23,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

df_test_23$Q23_factor <- ordered(df_test_23$Q23,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_ordinal_23 <- polr(Q23_factor ~ age, data = df_train_23, Hess = TRUE, method = "logistic")
summary(modelo_ordinal_23)
```


```{r}
modelo_lineal <- lm(Q23 ~ age, data = df_train_23)
y_pred_train <- predict(modelo_lineal)
y_pred_final_train <- pmin(pmax(round(y_pred_train), 1), 5)
mae_train <- mean(abs(df_train_23$Q23 - y_pred_final_train))
aciertos_train <- mean(df_train_23$Q23 == y_pred_final_train)

y_pred_test <- predict(modelo_lineal, newdata = df_test_23)
y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)
mae_test <- mean(abs(df_test_23$Q23 - y_pred_final_test))
aciertos_test <- mean(df_test_23$Q23 == y_pred_final_test)
print("Resultados para modelo lineal truncado:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```


```{r}
y_pred_train <- as.numeric(predict(modelo_ordinal_23))
mae_train <- mean(abs(df_train_23$Q23 - y_pred_train))
aciertos_train <- mean(df_train_23$Q23 == y_pred_train)

y_pred_test <- as.numeric(predict(modelo_ordinal_23, newdata = df_test_23))
# y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)   innecesario, y_pred_test ya es un 'factor', no es como el modelo lineal
mae_test <- mean(abs(df_test_23$Q23 - y_pred_test))
aciertos_test <- mean(df_test_23$Q23 == y_pred_test)
print("Resultados para modelo ordinal logístico:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```

¿Y si en vez de medir la exactitud en base a qué porcentaje de la población el modelo predijo exactamente su respuesta (1 a 5), intentaramos medir a cuántos le acierta sobre si están al menos a favor o no? 

```{r}

# Verdadero sentimiento
grupo_real_train <- ifelse(df_train_23$Q23 %in% 4:5, "a_favor", "no_a_favor")
grupo_real_test  <- ifelse(df_test_23$Q23  %in% 4:5, "a_favor", "no_a_favor")

# Predicción de sentimiento
grupo_pred_train <- ifelse(y_pred_train %in% 4:5, "a_favor", "no_a_favor")
grupo_pred_test  <- ifelse(y_pred_test  %in% 4:5, "a_favor", "no_a_favor")

# 3. Exactitud en clasificar a favor vs no a favor
acierto_a_favor_train <- mean(grupo_real_train == grupo_pred_train)
acierto_a_favor_test  <- mean(grupo_real_test  == grupo_pred_test)

# 4. Mostrar todo
print("Resultados para modelo ordinal logístico:")
list(
  MAE_train = mae_train,
  MAE_test = mae_test,
  exactitud_train = aciertos_train,
  exactitud_test = aciertos_test,
  exactitud_grupo_train = acierto_a_favor_train,
  exactitud_grupo_test = acierto_a_favor_test
)
```

```{r}
library(rstanarm)
```
```{r}
modelo_bayes <- stan_polr(
  Q30_factor ~ age,
  data = df_train_30,
  prior = normal(0, 2),
  seed = 1,
  chains = 1,
  refresh = 1,
  iter = 50
)


modelo_bayes <- stan_polr(
  Q30_factor ~ age,
  data = df_train_30,
  prior = R2(location = 0.4, what = "mean"),  # ~equivale a varianza intermedia en β
  seed = 1,
  chains = 1,
  iter= 50,
  refresh = 5
)

for (i in (1:10)) {
  location_mod = i/25
  modelo_bayes <- stan_polr(
    Q30_factor ~ age,
    data = df_train_30,
    prior = R2(location = location_mod, what = "mean"),  # ~equivale a varianza intermedia en β
    seed = 1,
    chains = 1,
    iter= 50,
    refresh = 5
  )
  
  posterior <- as.data.frame(modelo_bayes)
  hist(posterior$age, breaks=40, main="Posterior de β (edad)")
}

```

