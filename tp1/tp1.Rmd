---
title: 'TP 1: Regresión ordinal'
author: "Nicolás Celie, Martín Peralta, Nicolás Ian Rozenberg"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries}
library(tidyverse)
library(MASS)
library(broom) 
library(janitor)   
library(rsample)
```

## Ejercicio 1

```{r libraries}
set.seed(123)

df <- read.csv2("data/data.csv", sep="\t") %>%
  as.data.frame()

# Filtramos outliers
Q1 <- quantile(df$age, 0.25, na.rm = TRUE)
Q3 <- quantile(df$age, 0.75, na.rm = TRUE)
IQR <- Q3 - Q1

limite_inferior <- Q1 - 1.5 * IQR
limite_superior <- Q3 + 1.5 * IQR

nrow(df %>% 
  filter(age>limite_superior | age<limite_inferior)
  )

df <- df %>% 
  filter(age<limite_superior & age>limite_inferior)

# Split stratificado
df$strata <- interaction(df$religion, df$gender, df$education, drop = TRUE)

split <- initial_split(df, prop = 0.8, strata = strata)

df_train <- training(split)
df_test  <- testing(split)
```

## Ejercicio 2

Se eligió la pregunta Q30: 

## Ejercicio 3

Una forma de plantear el problema es mediante una regresión lineal donde la variable dependiente sea la respuesta en la escala Likert, y las covariables la edad y variables indicadoras del género. Es decir,

$$
Likert_i = \beta_{\text{intercept}} + \beta_{\text{masc}} \text{masc}_i + \beta_{\text{fem}} \text{fem}_i + \beta_{\text{edad}} \text{edad}_i + \epsilon_i
$$


El problema de este enfoque es que $Likert_i$ toma $1,...,5$ como posibles valores, pero modelo lineal común asume normalidad en los errores, y por lo tanto en la variable dependiente. En este caso, es una variable discreta con un rango de sólo 5 valores. Por otra parte, se asume que las distancias entre las respuestas son iguales. Por ejemplo, podría no tener sentido considerar que la diferencia entre "Totalmente en desacuerdo" y "En desacuerdo" sea la misma que "En desacuerdo" y "Neutro".


Otra forma es modelarlo mediante una regresión multinomial. En esta, se modela a la probabilidad de que el individuo $i$ responda la opción $j$ como

$$
\mathbb{P}(Likert_{i, j} = 1) = \text{Softmax}(z_i)_j
$$

donde $z_i$ es un vector en $\mathbb{R}^5$ tal que, para $1 \leq j \leq 5$,

$$
z_{i, j} = \beta_{\text{intercept, j}} + \beta_{\text{masc}, j} \text{masc}_i + \beta_{\text{fem}, j} \text{fem}_i + \beta_{\text{edad}, j} \text{edad}_i
$$
y $\text{Softmax} : \mathbb{R}^p \to \mathbb{R}^p$ es la función

$$
\text{Softmax}(z)_j = \frac{e^{z_j}}{\sum_{j=1}^{p} e^{z_j}}
$$
Sin embargo, dicho enfoque no es el más apropiado tampoco. Esto se debe a que la variable dependiente no es una variable categórica nominal. Las posibles respuestas tienen un orden, que no se está modelando.


## Ejercicio 4

TODO: Explicar en palabras


Supongamos que la variable respuesta ordinal \( Y \in \{1, 2, \dots, K\} \) surge de una variable latente continua \( Y^* \in \mathbb{R} \), que no observamos directamente, pero que está determinada por un modelo lineal:

$$
Y^* = \mathbf{x}^\top \boldsymbol{\beta} + \varepsilon
$$

donde \( \mathbf{x} \) representa el vector de covariables, \( \beta \) los coeficientes del modelo, y \( \varepsilon \sim \text{Logística}(0,1) \) es un término de error con distribución logística estándar.

La variable observada \( Y \) se define a partir de la latente \( Y^* \) mediante una función partida que utiliza umbrales (o puntos de corte) ordenados:

$$
Y = 
\begin{cases}
1 & \text{si } Y^* \leq \theta_1 \\
2 & \text{si } \theta_1 < Y^* \leq \theta_2 \\
\vdots \\
K & \text{si } Y^* > \theta_{K-1}
\end{cases}
$$

Estos umbrales satisfacen la condición de orden creciente:

$$
\theta_1 < \theta_2 < \cdots < \theta_{K-1}
$$

El modelo logit acumulativo se expresa entonces como:

$$
P(Y \leq j) = \sigma( \theta_j - \mathbf{x}^\top \boldsymbol{\beta}) \quad \text{para } j = 1, \dots, K-1
$$
## Ejercicio 5


```{r}
df_train_30 <- df_train %>% 
  filter(Q30 != 0)

df_test_30 <- df_test %>% 
  filter(Q30 != 0)

#Ordenamos en escala
df_train_30$Q30_factor <- ordered(df_train_30$Q30,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

df_test_30$Q30_factor <- ordered(df_test_30$Q30,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_ordinal <- polr(Q30_factor ~ age, data = df_train_30, Hess = TRUE, method = "logistic")
summary(modelo_ordinal)
```
## Ejercicio 6

```{r}

# Entrenamos con todos los datos en este caso
df_9 <- df %>% 
  filter(Q9 != 0)

df_9$Q9 <- ordered(df_9$Q9,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_9 <- polr(Q9 ~ age, data = df_9, Hess = TRUE, method = "logistic")
summary(modelo_9)
```
La función `predict` estima la probabilidad puntual de cada opción.

```{r}
probabilidades <- predict(modelo_9, newdata = data.frame(age = 25), type = "probs")
print(probabilidades)
```
TODO: Sumar 'De acuerdo' con 'Muy de acuerdo'

## Ejercicio 7

```{r}
mae <- function(y_true, y_pred) {
  return(mean(abs(y_true - y_pred)))
}
```

## Ejercicios 8 y 9



```{r}
modelo_lineal <- lm(Q30 ~ age, data = df_train_30)
y_pred_train <- predict(modelo_lineal)
y_pred_final_train <- pmin(pmax(round(y_pred_train), 1), 5)
mae_train <- mean(abs(df_train_30$Q30 - y_pred_final_train))
aciertos_train <- mean(df_train_30$Q30 == y_pred_final_train)

y_pred_test <- predict(modelo_lineal, newdata = df_test_30)
y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)
mae_test <- mean(abs(df_test_30$Q30 - y_pred_final_test))
aciertos_test <- mean(df_test_30$Q30 == y_pred_final_test)
print("Resultados para modelo lineal truncado:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```

```{r}
y_pred_train <- as.numeric(predict(modelo_ordinal))
mae_train <- mean(abs(df_train_30$Q30 - y_pred_train))
aciertos_train <- mean(df_train_30$Q30 == y_pred_train)

y_pred_test <- as.numeric(predict(modelo_ordinal, newdata = df_test_30))
# y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)   innecesario, y_pred_test ya es un 'factor', no es como el modelo lineal
mae_test <- mean(abs(df_test_30$Q30 - y_pred_test))
aciertos_test <- mean(df_test_30$Q30 == y_pred_test)
print("Resultados para modelo ordinal logístico:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```

Los modelos tuvieron un mal desempeño. Además de que obviamente, solo tienen una variable para predecir, lo más probable es que esté pasando una de estas dos cosas (o ambas): o hay poca varianza en las respuestas a 'Q30' o la edad no tiene nada que ver con 'Q30'.
Con esto en mente, decidimos ver qué porcentaje de personas respondió cada valor de la escala para 'Q30' y también visualizar la distribución de las edades de las personas que al menos estaban de acuerdo vs las que no.

```{r}
# Tabulación y porcentaje
tabla <- table(df$Q30)
porcentaje <- prop.table(tabla) * 100
porcentaje_formateado <- round(porcentaje, 2)
data.frame(Respuesta = names(tabla), Porcentaje = porcentaje_formateado)
```


```{r}
df$Q30_grupo <- ifelse(df$Q30 %in% 4:5, "De acuerdo (4 o 5)", "No de acuerdo (1 a 3)")
df_box <- df[!is.na(df$age) & !is.na(df$Q30_grupo), ]

# Boxplot
boxplot(age ~ Q30_grupo, data = df_box,
        col = c("#FFA07A", "#90EE90"),
        main = "Distribución de edad por respuesta a Q30",
        ylab = "Edad",
        xlab = "Grupo de respuesta")
```

Acá se ve claramente cómo casi no hay una diferencia clara por edad. Por lo tanto, es razonable que ninguno de los dos modelos (el lineal truncado y el ordinal) de buenos resultados.
Ahora, podemos buscar alguna pregunta que sí muestre una diferencia más contundente por edad. Para eso, podemos correr este programa (en vez de mirar un boxplots por cada pregunta) que nos ordene a las preguntas en base a la diferencia de edad que tiene la población de acuerdo y la que no. Para comparar las edades de las poblaciones, vamos a intentar únicamente comparando medias (puede que no haga falta más que esto).

```{r}
max_dif <- 0
max_dif_col <- ""

for (col in colnames(df)[1:44]) {
  df$grupos <- ifelse(df[[col]] %in% 4:5, "De acuerdo", "No de acuerdo")
  df_box <- df[!is.na(df$age) & !is.na(df[[col]]), ]
  media_dif <- abs(mean(df_box$age[df_box$grupos == "De acuerdo"]) - mean(df_box$age[df_box$grupos == "No de acuerdo"]))
  if (media_dif > max_dif) {
    max_dif_col <- col
    max_dif <- media_dif
  }
}

print(max_dif_col)
print(max_dif)

```

El programa indica que la pregunta Q23 ("I playfully insult my friends") maximiza la diferencia de medias de las poblaciones a favor y en contra. Veámoslo en el mismo gráfico que hicimos para Q30.

```{r}
df$Q23_grupo <- ifelse(df$Q23 %in% 4:5, "De acuerdo (4 o 5)", "No de acuerdo (1 a 3)")
df_box <- df[!is.na(df$age) & !is.na(df$Q23_grupo), ]

boxplot(age ~ Q23_grupo, data = df_box,
        col = c("#FFA07A", "#90EE90"),
        main = "Distribución de edad por respuesta a Q23",
        ylab = "Edad",
        xlab = "Grupo de respuesta")

table(df$Q23_grupo)
```
Podemos observar que ahora sí hay una diferencia entre las edades de las poblaciones de acuerdo y en desacuerdo. Veamos si los modelos predicen mejor la adhesión o no a esta pregunta 'Q23'.
```{r}
df_train_23 <- df_train %>% 
  filter(Q23 != 0)

df_test_23 <- df_test %>% 
  filter(Q23 != 0)

# Ordenamos en escala
df_train_23$Q23_factor <- ordered(df_train_23$Q23,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

df_test_23$Q23_factor <- ordered(df_test_23$Q23,
                  levels = 1:5,
                  labels = c("Muy en desacuerdo", "En desacuerdo", "Neutral", "De acuerdo", "Muy de acuerdo"))

modelo_ordinal_23 <- polr(Q23_factor ~ age, data = df_train_23, Hess = TRUE, method = "logistic")
summary(modelo_ordinal_23)
```


```{r}
modelo_lineal <- lm(Q23 ~ age, data = df_train_23)
y_pred_train <- predict(modelo_lineal)
y_pred_final_train <- pmin(pmax(round(y_pred_train), 1), 5)
mae_train <- mean(abs(df_train_23$Q23 - y_pred_final_train))
aciertos_train <- mean(df_train_23$Q23 == y_pred_final_train)

y_pred_test <- predict(modelo_lineal, newdata = df_test_23)
y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)
mae_test <- mean(abs(df_test_23$Q23 - y_pred_final_test))
aciertos_test <- mean(df_test_23$Q23 == y_pred_final_test)
print("Resultados para modelo lineal truncado:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```


```{r}
y_pred_train <- as.numeric(predict(modelo_ordinal_23))
mae_train <- mean(abs(df_train_23$Q23 - y_pred_train))
aciertos_train <- mean(df_train_23$Q23 == y_pred_train)

y_pred_test <- as.numeric(predict(modelo_ordinal_23, newdata = df_test_23))
# y_pred_final_test <- pmin(pmax(round(y_pred_test), 1), 5)   innecesario, y_pred_test ya es un 'factor', no es como el modelo lineal
mae_test <- mean(abs(df_test_23$Q23 - y_pred_test))
aciertos_test <- mean(df_test_23$Q23 == y_pred_test)
print("Resultados para modelo ordinal logístico:")
list(MAE_train = mae_train, MAE_test = mae_test, exactitud_train = aciertos_train, exactitud_test = aciertos_test)
```

¿Y si en vez de medir la exactitud en base a qué porcentaje de la población el modelo predijo exactamente su respuesta (1 a 5), intentaramos medir a cuántos le acierta sobre si están al menos a favor o no? 

```{r}

# Verdadero sentimiento
grupo_real_train <- ifelse(df_train_23$Q23 %in% 4:5, "a_favor", "no_a_favor")
grupo_real_test  <- ifelse(df_test_23$Q23  %in% 4:5, "a_favor", "no_a_favor")

# Predicción de sentimiento
grupo_pred_train <- ifelse(y_pred_train %in% 4:5, "a_favor", "no_a_favor")
grupo_pred_test  <- ifelse(y_pred_test  %in% 4:5, "a_favor", "no_a_favor")

# 3. Exactitud en clasificar a favor vs no a favor
acierto_a_favor_train <- mean(grupo_real_train == grupo_pred_train)
acierto_a_favor_test  <- mean(grupo_real_test  == grupo_pred_test)

# 4. Mostrar todo
print("Resultados para modelo ordinal logístico:")
list(
  MAE_train = mae_train,
  MAE_test = mae_test,
  exactitud_train = aciertos_train,
  exactitud_test = aciertos_test,
  exactitud_grupo_train = acierto_a_favor_train,
  exactitud_grupo_test = acierto_a_favor_test
)
```

```{r}
library(rstanarm)
```
```{r}
modelo_bayes <- stan_polr(
  Q30_factor ~ age,
  data = df_train_30,
  prior = normal(0, 2),
  seed = 1,
  chains = 1,
  refresh = 1,
  iter = 50
)


modelo_bayes <- stan_polr(
  Q30_factor ~ age,
  data = df_train_30,
  prior = R2(location = 0.4, what = "mean"),  # ~equivale a varianza intermedia en β
  seed = 1,
  chains = 1,
  iter= 50,
  refresh = 5
)

for (i in (1:10)) {
  location_mod = i/25
  modelo_bayes <- stan_polr(
    Q30_factor ~ age,
    data = df_train_30,
    prior = R2(location = location_mod, what = "mean"),  # ~equivale a varianza intermedia en β
    seed = 1,
    chains = 1,
    iter= 50,
    refresh = 5
  )
  
  posterior <- as.data.frame(modelo_bayes)
  hist(posterior$age, breaks=40, main="Posterior de β (edad)")
}

```

